---
title: "[STATS 412] Statistical Modeling of House Prices"
author: "Tianshu Fan, Jaehee Jeong, Lisa Kaunitz"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Data
```{r wrap-hook}
# urlPackage <- 'https://cran.r-project.org/src/contrib/Archive/randomForest/randomForest_4.6-14.tar.gz'
# install.packages(urlPackage, repos=NULL, type="source") 
library(readr)
library(dplyr)
library(ggplot2)
library(DataExplorer)
library(lubridate)
library(caTools)
library(MASS)
library(leaps)
library(caret)
library(pcr)
library(pls)
library(Metrics)
library(dplyr)
library(randomForest)
library(data.table)
library(leaps)
library(caTools)
library(randomForest)
library(glmnet) #cv.glmnet
```

### Basic EDA
```{r}
set.seed(1)

house <- read.csv('house.csv')
head(house)
dim(house)
num_ob_bf_drop <- dim(house)[1]

# Add a feature if there is a basement then 1 else 0
for(i in 1: nrow(house)){
    if (house$sqft_basement[i] >0) {
  house$sqft_basement_yesno[i] <- 1
  } else {
  house$sqft_basement_yesno[i] <- 0
  }
}

# Distribution of Date
ggplot(house, aes(x=date, y = price))+
  geom_line()+
  xlab('Date')+
  ylab('Price')+
  ggtitle('House Prices Over Time') + 
  theme_bw()

# Boxplot of prices
ggplot(house)+
  aes(x=price)+
  geom_boxplot() +
  ggtitle("Boxplot of Price") +
  theme_bw() +
  scale_x_continuous(labels = function(x) format(x, scientific = FALSE))

# Distribution of price
ggplot(house)+
  aes(x=price)+
  geom_histogram(col = 'black', bins = 20) +
  ggtitle("Distribution of Price (no transformation)") +
  theme_bw() +
  scale_x_continuous(labels = function(x) format(x, scientific = FALSE))

# Get rid of outliers (price-wise)
summary(house$price)
first_quartile <- summary(house$price)[[2]]
third_quartile <- summary(house$price)[[5]]
IQR <- third_quartile-first_quartile
Upper <- 1.5*IQR + third_quartile
Lower <- first_quartile - 1.5*IQR
house <- subset(house, price >= Lower & price <= Upper)

# Distribution of Date
ggplot(house, aes(x=date, y = price))+
  geom_line()+
  xlab('Date')+
  ylab('Price')+
  ggtitle('House Prices Over Time') + 
  theme_bw()

# Boxplot of prices
ggplot(house)+
  aes(x=price)+
  geom_boxplot() +
  ggtitle("Boxplot of Price") +
  theme_bw() +
  scale_x_continuous(labels = function(x) format(x, scientific = FALSE))

# Distribution of price
ggplot(house)+
  aes(x=price)+
  geom_histogram(col = 'black', bins = 20) +
  ggtitle("Distribution of Price (no transformation)") +
  theme_bw() +
  scale_x_continuous(labels = function(x) format(x, scientific = FALSE))

# Distribution of price using log transform 
house$log_price <- log(house$price)
ggplot(house)+
  aes(x=log_price)+
  geom_histogram(col = 'black', bins = 20) +
  ggtitle("Distribution of Price (log transformation)") +
  theme_bw()

# Drop date: No relationshop is detected
# Drop id: No meaning
# Drop zipcode: We have latitude and longitude info
# Drop sqft_basement: I have sqft_basement_yesno feature
drop <- c('date','id','zipcode','sqft_basement','log_price')
house <- house[!names(house) %in% drop]
summary(house)
dim(house)
num_ob_af_drop <- dim(house)[1]

num_ob_af_drop/num_ob_bf_drop*100

#DataExplorer::create_report(house)
```

### Using ridge and lasso to do feature selection
```{r}
set.seed(101)
# Create a ridge model
lambda = lambda=seq(0.001,0.1, by=0.001)

ridge_model <- cv.glmnet(as.matrix(house[,-1]), house$price, alpha = 0
                      , lambda = lambdas_to_try
                      ,standardize = TRUE, nfolds =10)

ridge_model$lambda.min

best_lambda_ridge <- ridge_model$lambda.1se  # largest lambda in 1 SE
ridge_coef <- ridge_model$glmnet.fit$beta[,  # retrieve coefficients
              ridge_model$glmnet.fit$lambda  # at lambda.1se
              == best_lambda_ridge]
coef_r = data.table(ridge = ridge_coef)      # build table
coef_r[, feature := names(ridge_coef)]       # add feature names
to_plot_r = melt(coef_r                      # label table
               , id.vars='feature'
               , variable.name = 'model'
               , value.name = 'coefficient')
ggplot(data=to_plot_r,                       # plot coefficients
       aes(x=feature, y=coefficient, fill=model)) +
       coord_flip() +         
       geom_bar(stat='identity', fill='brown4', color='blue') +
       facet_wrap(~ model) + guides(fill=FALSE) 

summary(ridgeMod)


# Create a LASSO model
lasso_model <- cv.glmnet(as.matrix(house[,-1]), house$price, alpha = 1,
                      lambda = lambdas_to_try,
                      standardize = TRUE, nfolds = 10)
lasso_model$lambda.min

best_lambda_lasso <- lasso_model$lambda.1se  # largest lambda in 1 SE
lasso_coef <- lasso_model$glmnet.fit$beta[,  # retrieve coefficients
              lasso_model$glmnet.fit$lambda  # at lambda.1se
              == best_lambda_lasso]
coef_l = data.table(lasso = lasso_coef)      # build table
coef_l[, feature := names(lasso_coef)]       # add feature names
to_plot_l = melt(coef_l                      # label table
               , id.vars='feature'
               , variable.name = 'model'
               , value.name = 'coefficient')
ggplot(data=to_plot_l,                       # plot coefficients
       aes(x=feature, y=coefficient, fill=model)) +
       coord_flip() +         
       geom_bar(stat='identity', fill='brown4', color='blue') +
       facet_wrap(~ model) + guides(fill=FALSE) 

all_coef = data.table (lasso = lasso_coef,
                   ridge = ridge_coef)

```

### Creating randomForest Model to know important features
```{r}
house.rf <- randomForest(price ~ ., data = house,
                         importance = TRUE)
print(house.rf)
import <- house.rf$importance
import
```

### Save only important feaatures
```{r}
keep <- c('price','lat','sqft_living','grade','sqft_living15','sqft_above','long','yr_built','sqft_lot15','sqft_lot','bathrooms')
house <- house[names(house) %in% keep]
summary(house)
```

### We decided not to convert the numerical variables to a factor 
```{r}
# house$bathrooms = as.factor(house$bathrooms)
# house$grade = as.factor(house$grade)
```

### Split dateset to a train set and a test set
```{r}
s = sort(sample(nrow(house), nrow(house)*.7))
train <- house[s,]
test <- house[-s,]

# Create a rmse function to test results
rmse <- function(y_hat, y) sqrt(mean((y_hat - y)^2))
```

### Create linear Models
```{r}
lMod <- lm(price~., data=train)
summary(lMod)
rmse(test$price, predict(lMod,test[-1]))

# Use step function
lstepMod <- step(lMod)
summary(lstepMod)
rmse(test$price, predict(lstepMod,test[-1]))
```

### Create a randomforest model
```{r}
rfMod <- randomForest(price ~ ., data = train,
                         importance = TRUE)
print(rfMod)
rmse(test$price, predict(rfMod, test[-1]))
rfMod$importance
```


# Create PCR models
```{r}
set.seed(27)
pc <- prcomp(house, scale = T)
summary(pc)

sort(round(pc$rotation[,1], 2))

# PCR
pcrMod <- pcr(price ~ ., data = train, ncomp = 5)

rmse(predict(pcrMod, nncomp = 5), train$price) # RMSE = 173611.4
rmse(predict(pcrMod, nncomp = 5), test$price) # RMSE =  219128.2

pcrmse <- RMSEP(pcrMod, newdata = test)
plot(pcrmse, main = "")
which.min(pcrmse$val) # 6 pc
pcrmse$val[6] # 153961.9

# I couldn't find pcrMod_2. Did I delete something?
#pcrCV <- RMSEP(pcrMod_2, estimate = "CV")
#plot(pcrmse, main = "PCR vs RMSE")
```

# Create Ridge/LASSO models
```{r}
set.seed(101)
# Create a ridge model
lambdas_to_try = lambda=seq(0.001,0.1, by=0.001)

ridgeMod <- cv.glmnet(as.matrix(train[,-1]), train$price, alpha = 0
                      , lambda = lambdas_to_try
                      ,standardize = TRUE, nfolds =10)
plot(ridgeMod$glmnet.fit,xvar = "lambda", label = T)
plot(ridgeMod)
ridgeMod$lambda.min

# Create a LASSO model
lassoMod <- cv.glmnet(as.matrix(train[,-1]), train$price, alpha = 1,
                      lambda = lambdas_to_try,
                      standardize = TRUE, nfolds = 10)
plot(lassoMod$glmnet.fit,xvar = "lambda", label = T)
lassoMod$lambda.min
```
### Create a df showing all the rmse values
```{r}
rmse_colnames<-c("Model1-lMod","Model2-lstepMod","Model3-rfMod", "Model4-pcrMod"
                 , "Model5-Ridge", "Model6-Lasso")
rmse_result <-c( rmse(predict(lMod, test), test$price)
             ,rmse(predict(lstepMod, test), test$price)
             ,rmse(predict(rfMod, test), test$price)
             ,rmse(predict(pcrMod, nncomp = 5), test$price) 
             ,rmse(predict(ridgeMod, newx = as.matrix(test[,-1])
                  , s=ridgeMod$lambda.min)
                  ,test$price)
             ,rmse(predict(lassoMod, newx = as.matrix(test[,-1])
                  , s=lassoMod$lambda.min)
                  ,test$price)
             )

result_df <- data.frame(rmse_colnames,rmse_result)
result_df
```


### Add a residual plot of rfMod
```{r}
#Find residuals by subtracting predicted from acutal values
err <- rfMod$predicted - train$price

#Make data frame holding residuals and fitted values
df <- data.frame(Residuals=err, Fitted.Values=rfMod$predicted)

#Sort data by fitted values
df2 <- df[order(df$Fitted.Values),]

#Create plot
plot(Residuals~Fitted.Values, data=df2)

#Add origin line at (0,0) with grey color #8
abline(0,0, col=8)

#Add the same smoothing line from lm regression with color red #2
lines(lowess(df2$Fitted.Values, df2$Residuals), col=2)
```